## 一、项目简介（1. Repo’s Brief Introduction）

这里是截至目前关于GPT的一切你应该知道的，包括论文，开源模型，网站，博客... 我将它们整理到一起，以便大家更方便地了解和使用GPT。项目不定期更新，内容可能不全，还请大家补充！

Here is everything about GPT  what you should know so far, including papers, open source models, websites, blogs... I organize them together so that everyone can understand and use GPT more easily. The project is updated from time to time, and the content may be incomplete. Please add it!

## 二、ChatGPT(GPT4)的前世今生（The past and present of ChatGPT (GPT4)）

### 1. GPT(Generated Pretrained Transformer)(生成式预训练模型)

#### 1.1 GPT1

paper:[Improving Language Understanding by Generative Pretraining](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

code:[openai/finetune-transformer-lm](https://github.com/openai/finetune-transformer-lm)

#### 1.2 GPT2

paper:[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

code:[openai/gpt-2](https://github.com/openai/gpt-2)

#### 1.3 GPT3

paper:[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165v4)

code:未开源(Not open source)

#### 1.4 GPT3.5(InstructionGPT)

paper(有两篇供参考)(There are two ones to be referred):

[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)																		  

[Learning to summarize from human feedback](https://arxiv.org/abs/2009.01325)

code:未开源(Not open source)

#### 1.4 GPT4

paper:[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

code:未开源(Not open source)

### 2.Embeedings

#### 2.1 Word2Vec

paper:[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

code:[Tensorflow Core/Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec)

#### 2.2 Doc2Vec

paper:[Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)

code:[Gensim/Doc2Vec Model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html)

(Gensim is a popular python nlp processing library)

#### 2.3 Context2Vec

paper:[context2vec: Learning Generic Context Embedding with Bidirectional LSTM](https://aclanthology.org/K16-1006.pdf)

code:[orenmel/context2Vec](https://github.com/orenmel/context2vec)

#### 2.4 lda2Vec

paper:[Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec](https://arxiv.org/abs/1605.02019v1)

code:[cemoody/lda2vec](https://github.com/cemoody/lda2vec)

#### 2.5 TWEC

paper:[Training Temporal Word Embeddings with a Compass](https://arxiv.org/abs/1906.02376v1)

code:[valedica/twec](https://github.com/valedica/twec)

#### 2.6 USE

paper:[Multilingual Universal Sentence Encoder for Semantic Retrieval](https://arxiv.org/abs/1907.04307v1)

code:未找到(Failed to find)

#### 2.7 fastText

paper:[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606v2)

code:[facebookresearch/fastText](https://github.com/facebookresearch/fastText)

#### 2.8 ELMo

paper:[Deep contextualized word representations](https://arxiv.org/abs/1802.05365v2)

code:[HIT-SCIR/ELMoForManyLangs](https://github.com/HIT-SCIR/ELMoForManyLangs)

#### 2.9 GloVe

paper:[GloVe:Global Vectors for Word Representation](https://www-nlp.stanford.edu/pubs/glove.pdf)

code:[stanfordnlp/GloVe](https://github.com/stanfordnlp/GloVe)



